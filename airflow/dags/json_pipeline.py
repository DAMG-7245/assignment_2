from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from datetime import datetime, timedelta
import os

# --------------------------------
# é»˜è®¤å‚æ•°è®¾ç½®
# --------------------------------
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 2, 12),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# å®šä¹‰ DAG
dag = DAG(
    'json_pipeline',
    default_args=default_args,
    description='JSON Financial Data Pipeline DAG',
    schedule_interval='@daily',
    catchup=False
)

# --------------------------------
# 1. æ¼”ç¤ºä½¿ç”¨ä¸Šä¸‹æ–‡å‚æ•°çš„ Python ä»»åŠ¡
# --------------------------------
def print_context(**context):
    """Shows how to use context variables in Airflow"""
    print(f"Execution date is {context['ds']}")
    print(f"Task instance: {context['task_instance']}")
    return "Hello from first Json task!"

task1 = PythonOperator(
    task_id='print_context',
    python_callable=print_context,
    dag=dag
)

# --------------------------------
# 2. è§£åŽ‹æ–‡ä»¶
# --------------------------------
def unzip_data(**context):
    os.system("unzip -o ../../2021q4/2021q4.zip -d ../../2021q4/extracted/")
    print("âœ… Data unzipped successfully")

unzip_task = PythonOperator(
    task_id='unzip_sec_data',
    python_callable=unzip_data,
    dag=dag
)

# --------------------------------
# 3. è½¬æ¢ CSV -> JSON
# --------------------------------
def transform_to_json(**context):
    # å‡è®¾ data_ingestion/sec_csv_reader.py å­˜åœ¨å¹¶æ‰§è¡Œè½¬æ¢é€»è¾‘
    os.system("python data_ingestion/sec_csv_reader.py")
    print("âœ… JSON transformation complete")

transform_task = PythonOperator(
    task_id='transform_to_json',
    python_callable=transform_to_json,
    dag=dag
)

# --------------------------------
# 4. åˆ›å»º JSON_SCHEMA
# --------------------------------
create_schema = SnowflakeOperator(
    task_id='create_json_schema',
    snowflake_conn_id='snowflake_default',
    sql="""
    -- åˆ‡æ¢åˆ° DBT_DB æ•°æ®åº“ï¼ˆè‹¥æ²¡å»ºå¥½ï¼Œéœ€è¦å…ˆå»ºæ•°æ®åº“ï¼‰
    USE DATABASE DBT_DB;
    -- å¦‚æžœ JSON_SCHEMA ä¸å­˜åœ¨åˆ™åˆ›å»º
    CREATE SCHEMA IF NOT EXISTS JSON_SCHEMA;
    """,
    dag=dag
)

# --------------------------------
# 5. åœ¨ JSON_SCHEMA ä¸‹åˆ›å»ºè¡¨
# --------------------------------
create_table = SnowflakeOperator(
    task_id='create_json_table',
    snowflake_conn_id='snowflake_default',
    sql="""
    USE SCHEMA DBT_DB.JSON_SCHEMA;
    CREATE TABLE IF NOT EXISTS json_sec_data (
        cik STRING,
        company_name STRING,
        filing_date DATE,
        fiscal_year INT,
        adsh STRING,
        tag STRING,
        value FLOAT,
        unit STRING,
        data VARIANT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """,
    dag=dag
)

# --------------------------------
# 6. åŠ è½½ JSON æ•°æ®åˆ° Snowflake
# --------------------------------
def load_to_snowflake(**context):
    # å‡è®¾ storage/snowflake_loader_json.py ä¸­æ‰§è¡Œ COPY INTO æˆ–ç±»ä¼¼é€»è¾‘
    os.system("python storage/snowflake_loader_json.py")
    print("âœ… JSON loaded into Snowflake")

load_task = PythonOperator(
    task_id='load_to_snowflake',
    python_callable=load_to_snowflake,
    dag=dag
)

# --------------------------------
# 7. è¿è¡Œ dbt models
# --------------------------------
run_dbt = BashOperator(
    task_id='run_dbt_models',
    bash_command="""
        echo "ðŸ‘‰ Running dbt..."
        dbt run --profile data_pipeline --project-dir /opt/airflow/dbt/data_pipeline
    """,
    # å…³é”®ä¿®æ­£ï¼šé€šè¿‡ env å‚æ•°ä¼ é€’çŽ¯å¢ƒå˜é‡
    env={'DBT_PROFILES_DIR': '/opt/airflow/dbt/data_pipeline'},
    dag=dag
)

# --------------------------------
# 8. è¿è¡Œ dbt tests
# --------------------------------
test_dbt = BashOperator(
    task_id='test_dbt_models',
    bash_command="echo 'ðŸ‘‰ Testing dbt models...' && dbt test --profile data_pipeline --project-dir /opt/airflow/dbt/data_pipeline",
    
    env={'DBT_PROFILES_DIR': '/opt/airflow/dbt/data_pipeline'},  
    dag=dag
)


# --------------------------------
# è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
# --------------------------------
# å¯æ ¹æ®éœ€è¦æ”¹å˜å…ˆåŽé¡ºåºï¼Œä»¥ä¸‹ä»…æ˜¯ç¤ºä¾‹
task1 >> unzip_task >> transform_task 
transform_task >> create_schema >> create_table >> load_task >> run_dbt >> test_dbt
